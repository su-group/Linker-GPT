# build_vocab_from_csvs.py
import pandas as pd
import re
import json
import argparse
import os

def build_vocab_from_csvs(csv_paths, smiles_col='smiles', scaffold_col='scaffold_smiles', output_dir='.'):
    """
    ä»å¤šä¸ª CSV æ–‡ä»¶æ„å»ºç»Ÿä¸€çš„ SMILES token è¯æ±‡è¡¨ï¼Œå¹¶å¼ºåˆ¶åŒ…å«å›ºå®š tokenã€‚
    """
    # ğŸ”‘ æ–°å¢ï¼šå›ºå®šå¿…é¡»åŒ…å«çš„ tokensï¼ˆå³ä½¿æ•°æ®ä¸­æ²¡æœ‰ï¼‰
    fixed_tokens = {'<', '>', '[', ']', '#'}  # ä½ å¯ä»¥æŒ‰éœ€å¢åˆ 

    # 1. å®šä¹‰ä¸ molgpt ä¸€è‡´çš„ tokenization æ­£åˆ™è¡¨è¾¾å¼
    pattern = r"(\[[^\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])"
    regex = re.compile(pattern)
    
    all_tokens = set()
    total_smiles = 0

    for csv_path in csv_paths:
        print(f"Processing {csv_path}...")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        df = pd.read_csv(csv_path)
        df.columns = df.columns.str.lower()

        smiles_candidates = [col for col in df.columns if 'smile' in col]
        if not smiles_candidates:
            raise ValueError(f"No SMILES column found in {csv_path}. Looked for columns containing 'smile'.")
        actual_smiles_col = smiles_candidates[0]
        print(f"  -> Using SMILES column: '{actual_smiles_col}'")

        smiles_series = df[actual_smiles_col].dropna().astype(str)
        total_smiles += len(smiles_series)

        for smi in smiles_series:
            tokens = regex.findall(smi)
            all_tokens.update(tokens)

        if scaffold_col and scaffold_col.lower() in df.columns:
            scaffold_series = df[scaffold_col.lower()].dropna().astype(str)
            for scaf in scaffold_series:
                tokens = regex.findall(scaf)
                all_tokens.update(tokens)
            print(f"  -> Also processed scaffold column: '{scaffold_col}'")

    # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šåˆå¹¶å›ºå®š token
    all_tokens = all_tokens.union(fixed_tokens)
    print(f"Added fixed tokens: {sorted(fixed_tokens)}")

    # 2. æ„å»ºè¯æ±‡è¡¨ï¼ˆæ’åºä»¥ç¡®ä¿å¯å¤ç°ï¼‰
    chars = sorted(list(all_tokens))  # æ’åºä¿è¯ stoi ç¨³å®š
    stoi = {ch: i for i, ch in enumerate(chars)}
    itos = {i: ch for i, ch in enumerate(chars)}

    # 3. ä¿å­˜
    os.makedirs(output_dir, exist_ok=True)
    stoi_path = os.path.join(output_dir, 'stoi.json')
    itos_path = os.path.join(output_dir, 'itos.json')

    with open(stoi_path, 'w', encoding='utf-8') as f:
        json.dump(stoi, f, indent=2)
    with open(itos_path, 'w', encoding='utf-8') as f:
        json.dump(itos, f, indent=2)

    print(f"\nâœ… Vocabulary built successfully!")
    print(f"   Total SMILES processed: {total_smiles}")
    print(f"   Vocabulary size: {len(chars)} (including {len(fixed_tokens)} fixed tokens)")
    print(f"   stoi saved to: {stoi_path}")
    print(f"   itos saved to: {itos_path}")
    print(f"\nSample tokens: {list(chars)[:15]} ...")

    return stoi, itos

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Build vocabulary from multiple CSV files containing SMILES.")
    parser.add_argument('--csv_files', nargs='+', required=True,
                        help="List of CSV file paths (e.g., data/QM9.csv data/ZINC.csv)")
    parser.add_argument('--smiles_col', type=str, default='smiles',
                        help="Name of the SMILES column (case-insensitive, default: 'smiles')")
    parser.add_argument('--scaffold_col', type=str, default='scaffold_smiles',
                        help="Name of the scaffold SMILES column (optional; set to '' to skip)")
    parser.add_argument('--output_dir', type=str, default='.',
                        help="Directory to save stoi.json and itos.json (default: current dir)")

    args = parser.parse_args()

    scaffold_col = args.scaffold_col if args.scaffold_col.strip() != '' else None

    build_vocab_from_csvs(
        csv_paths=args.csv_files,
        smiles_col=args.smiles_col,
        scaffold_col=scaffold_col,
        output_dir=args.output_dir
    )